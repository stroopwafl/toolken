# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/05_inference.ipynb.

# %% ../nbs/05_inference.ipynb 2
from __future__ import annotations
import math, random, torch, matplotlib.pyplot as plt, numpy as np, matplotlib as mpl, shutil, os, gzip, pickle, re, copy, time
from pathlib import Path
from functools import partial
import fastcore.all as fc
from glob import glob
import json

from torch import tensor, nn, optim
import torch.nn.functional as F
from datasets import load_dataset
from torch.optim import lr_scheduler
from torch.utils.data import DataLoader, default_collate
from torch.nn import init
from torch.nn.utils.rnn import pad_sequence
from typing import List, Optional

from datetime import datetime, timedelta
import calendar
from fastprogress import progress_bar
from einops import rearrange

from .model import *
from .tokenizer import *
from .datasets import *

# %% auto 0
__all__ = ['add', 'subtract', 'multiply', 'divide', 'generate', 'decode_toolken', 'format_args', 'complete_function_call',
           'sample']

# %% ../nbs/05_inference.ipynb 11
def add(x,y): return x+y
def subtract(x,y): return x-y
def multiply(x,y): return x*y
def divide(x,y): return x/y

# %% ../nbs/05_inference.ipynb 12
def generate(self, model, prompt, temperature=0.8, top_p=0.95, max_len=512, stop_token=[]):
    bsz = len(prompts)
    params = model.params
    assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)
    
    prompt_tokens = [tokenizer.encode(x, bos=True, eos=False) for x in prompts]

    min_prompt_size = min([len(t) for t in prompt_tokens])
    max_prompt_size = max([len(t) for t in prompt_tokens])

    total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)
    
    tokens = torch.full((bsz, total_len), tokenizer.pad_id).to(device).long()
    start_idx = len(prompt_tokens)
    prev_idx = 0

    for current_idx in range(start_idx, max_len):
        concat_logits = model(prompt_tokens[prev_idx:current_idx], prev_idx)
        # func_logits += self.logits_bias
        if temperature > 0:
            probs = torch.softmax(concat_logits, dim=-1)
            next_token = sample_top_p(probs, top_p)
        else:
            next_token = torch.argmax(logits, dim=-1)

        tokens[:, current_idx] = next_token
        prev_idx = current_idx

        if next_token >= 32000:
            break
        if next_token in stop_token:
            break
    return tokens, current_idx

# %% ../nbs/05_inference.ipynb 13
def decode_toolken(generated_tokens, current_idx, tokenizer, func_dict):
    decoded = []
    for i, t in list(generated_tokens[:current_idx]):
        try: decoded.append(tokenizer.decode(t))
        except IndexError: pass
        if t >= 32000: decoded.append(f'{[k for k,v in func_dict.items() if v == t-32000][0]}(')
    return decoded

# %% ../nbs/05_inference.ipynb 14
def format_args(cur_generation, func):
    """Converts raw argument formats from LLM to a list of arguments [x,y]"""
    args = cur_generation.split(func)[-1].replace("=", "").replace(">", "").replace("((", "(").replace("))", ")")
    args = args.replace("$", "")
    if ", " in args:
        args = args.replace(", ", ";").replace(",", "").replace(";", ", ")
    args = args.replace(" ", "")
    if "(" not in args or ")" not in args:
            raise Exception("invalid args")
    if '%' in args:
        temp = args.split("(")[1].split(")")[0].split(",")
        for arg_i, arg in enumerate(temp):
            if "%" in arg:
                arg = arg.replace("%", "").strip()
                arg = str(float(arg) / 100)
            temp[arg_i] = arg
        args = f"({', '.join(temp)})"
    args = [int(x) for x in args.replace("(", "").replace(")", "").split(',')]
    return args

# %% ../nbs/05_inference.ipynb 15
def complete_function_call(model, current_generation, func, func_template):
    # complete the arguments
    func_prompt = func_template.replace('[QUESTION]', q) + current_generation
    model.inference_mode = 'baseline'
    generated_func_tokens = generate(model, func_prompt, temperature=temperature, top_p=top_p, max_len=max_len, stop_token=[29897, 3892])
    current_generation += tokenizer.decode(list(generated_func_tokens))

    # extract args and do function call
    args = format_args(current_generation, func)
    func_name = func[1:-1]
    for f in func_list:
        if f.__name__ == func_name: res = f(*args)
    current_generation = current_generation.split(func)[0] + str(res)
    return current_generation

# %% ../nbs/05_inference.ipynb 17
def sample(model:callable, prompt:str, tokenizer, func_template:str, func_dict:dict, temperature=0.8, top_p=0.95, max_len=512):
    current_generation = ""
    while True:
        end_loop = True
        
        # generate up until the first toolken
        model.inference_mode = 'func_embedding'
        generated_tokens, current_idx = generate(model, prompt+current_generation, temperature=temperature, top_p=top_p, max_len=max_len)
        decoded = decode_toolken(generated_tokens, current_idx, tokenizer, func_dict)
        current_generation = "".join(decoded)
        
        # "tool mode" â€” make function calls if present
        for func in func_dict.keys():
            if decoded[-1] == func + "(":
                end_loop = False
                current_generation = complete_function_call(model, current_generation, func, func_template)
                
        if end_loop: break
    
    return current_generation
