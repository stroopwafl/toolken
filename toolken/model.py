# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_model.ipynb.

# %% auto 0
__all__ = ['ModelArgs', 'RMSNorm', 'precompute_freqs_cis', 'reshape_for_broadcast', 'apply_rotary_emb', 'LoRA', 'Attention',
           'FeedForward', 'TransformerBlock', 'Transformer', 'FunctionModel', 'sample_top_p', 'setup_model_parallel']

# %% ../nbs/01_model.ipynb 3
# Copyright (c) Meta Platforms, Inc. and affiliates.
# This software may be used and distributed according to the terms of the GNU General Public License version 3.

import os
from typing import Optional, Tuple
from dataclasses import dataclass
import math
from typing import List
import torch
from torch import nn
import torch.nn.functional as F
import copy
import random
import json
import fairscale.nn.model_parallel.initialize as fs_init
from fairscale.nn.model_parallel.layers import (
    ParallelEmbedding,
    RowParallelLinear,
    ColumnParallelLinear,
)
from fairscale.nn.model_parallel.mappings import scatter_to_model_parallel_region, gather_from_model_parallel_region
from fairscale.nn.model_parallel.initialize import initialize_model_parallel
import re
import fastcore.all as fc
from einops import rearrange

# %% ../nbs/01_model.ipynb 4
@dataclass
class ModelArgs:
    dim: int = 512
    n_layers: int = 8
    n_heads: int = 8
    vocab_size: int = -1  # defined later by tokenizer
    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2
    norm_eps: float = 1e-5

    max_batch_size: int = 32
    max_seq_len: int = 2048
    lora = False

# %% ../nbs/01_model.ipynb 5
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float()).type_as(x)
        return output * self.weight

# %% ../nbs/01_model.ipynb 6
def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    t = torch.arange(end, device=freqs.device)  # type: ignore
    freqs = torch.outer(t, freqs).float()  # type: ignore
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64
    return freqs_cis

# %% ../nbs/01_model.ipynb 7
def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):
    ndim = x.ndim
    assert 0 <= 1 < ndim
    assert freqs_cis.shape == (x.shape[1], x.shape[-1])
    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]
    return freqs_cis.view(*shape)

# %% ../nbs/01_model.ipynb 8
def apply_rotary_emb(
    xq: torch.Tensor,
    xk: torch.Tensor,
    freqs_cis: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor]:
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xk)

# %% ../nbs/01_model.ipynb 9
class LoRA(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.lora_alpha = 32
        self.r = 4
        self.scaling = self.lora_alpha / self.r

        self.lora_A = nn.Parameter(torch.zeros((self.r, input_dim)))
        self.lora_B = nn.Parameter(torch.zeros((output_dim, self.r)))
        self.reset_lora_parameters()

    def reset_lora_parameters(self):
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)

    def forward(self, x):
        return (x @ self.lora_A.T @ self.lora_B.T) * self.scaling

    def upgrade_state_dict_named(self, state_dict, name):
        
        prefix = name + '.lora_A'
        if prefix not in state_dict:
            state_dict[prefix] = self.lora_A

        prefix = name + '.lora_B'
        if prefix not in state_dict:
            state_dict[prefix] = self.lora_B

# %% ../nbs/01_model.ipynb 10
class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        
        self.n_local_heads = args.n_heads // fs_init.get_model_parallel_world_size()
        self.head_dim = args.dim // args.n_heads
        self.lora = args.lora

        self.wq = ColumnParallelLinear(
            args.dim,
            args.n_heads * self.head_dim,
            bias=False,
            gather_output=False,
            init_method=lambda x: x,
        )
        self.wk = ColumnParallelLinear(
            args.dim,
            args.n_heads * self.head_dim,
            bias=False,
            gather_output=False,
            init_method=lambda x: x,
        )
        self.wv = ColumnParallelLinear(
            args.dim,
            args.n_heads * self.head_dim,
            bias=False,
            gather_output=False,
            init_method=lambda x: x,
        )
        self.wo = RowParallelLinear(
            args.n_heads * self.head_dim,
            args.dim,
            bias=False,
            input_is_parallel=True,
            init_method=lambda x: x,
        )

        self.cache_k = torch.zeros(
            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)
        ).cuda()
        self.cache_v = torch.zeros(
            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)
        ).cuda()
        
        if self.lora:
            self.q_lora = LoRA(args.dim, args.dim)
            self.k_lora = LoRA(args.dim, args.dim)
            self.v_lora = LoRA(args.dim, args.dim)
                
    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):
        bsz, seqlen, _ = x.shape
        q, k, v = self.wq(x), self.wk(x), self.wv(x)
        
        if self.lora:
            q = gather_from_model_parallel_region(q) + self.q_lora(x)
            k = gather_from_model_parallel_region(k) + self.k_lora(x)
            v = gather_from_model_parallel_region(v) + self.v_lora(x)

            q = scatter_to_model_parallel_region(q)
            k = scatter_to_model_parallel_region(k)
            v = scatter_to_model_parallel_region(v)

        q = q.view(bsz, seqlen, self.n_local_heads, self.head_dim)
        k = k.view(bsz, seqlen, self.n_local_heads, self.head_dim)
        v = v.view(bsz, seqlen, self.n_local_heads, self.head_dim)

        q, k = apply_rotary_emb(q, k, freqs_cis=freqs_cis)

        self.cache_k = self.cache_k.to(q)
        self.cache_v = self.cache_v.to(q)
        
        cache_k = self.cache_k.detach().clone()
        cache_v = self.cache_v.detach().clone()
        cache_k[:bsz, start_pos : start_pos + seqlen] = k
        cache_v[:bsz, start_pos : start_pos + seqlen] = v
        self.cache_k = cache_k
        self.cache_v = cache_v

        # self.cache_k[:bsz, start_pos : start_pos + seqlen] = k
        # self.cache_v[:bsz, start_pos : start_pos + seqlen] = v

        keys = self.cache_k[:bsz, : start_pos + seqlen]
        values = self.cache_v[:bsz, : start_pos + seqlen]

        q = q.transpose(1, 2)
        keys = keys.transpose(1, 2)
        values = values.transpose(1, 2)
        scores = torch.matmul(q, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
        if mask is not None:
            scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)
        scores = F.softmax(scores.float(), dim=-1).type_as(q)
        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim)
        output = output.transpose(
            1, 2
        ).contiguous().view(bsz, seqlen, -1)

        return self.wo(output)

# %% ../nbs/01_model.ipynb 11
class FeedForward(nn.Module):
    def __init__(
        self,
        dim: int,
        hidden_dim: int,
        multiple_of: int,
    ):
        super().__init__()
        hidden_dim = int(2 * hidden_dim / 3)
        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)

        self.w1 = ColumnParallelLinear(
            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
        )
        self.w2 = RowParallelLinear(
            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x
        )
        self.w3 = ColumnParallelLinear(
            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
        )

    def forward(self, x):
        return self.w2(F.silu(self.w1(x)) * self.w3(x))

# %% ../nbs/01_model.ipynb 12
class TransformerBlock(nn.Module):
    def __init__(self, layer_id: int, args: ModelArgs):
        super().__init__()
        self.n_heads = args.n_heads
        self.dim = args.dim
        self.head_dim = args.dim // args.n_heads
        self.attention = Attention(args)
        self.feed_forward = FeedForward(
            dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of
        )
        self.layer_id = layer_id
        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)
        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)

    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):
        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis, mask)
        out = h + self.feed_forward.forward(self.ffn_norm(h))
        return out

# %% ../nbs/01_model.ipynb 13
class Transformer(nn.Module):
    def __init__(self, params: ModelArgs):
        super().__init__()
        self.params = params
        self.vocab_size = params.vocab_size
        self.n_layers = params.n_layers

        self.tok_embeddings = ParallelEmbedding(
            params.vocab_size, params.dim, init_method=lambda x: x
        )

        self.layers = torch.nn.ModuleList()
        for layer_id in range(params.n_layers):
            self.layers.append(TransformerBlock(layer_id, params))

        self.norm = RMSNorm(params.dim, eps=params.norm_eps)
        self.output = ColumnParallelLinear(
            params.dim, params.vocab_size, bias=False, init_method=lambda x: x
        )

        self.freqs_cis = precompute_freqs_cis(
            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2
        )

    # @torch.inference_mode()
    def forward(self, tokens: torch.Tensor, start_pos: int, last_logits_only=False):
        _bsz, seqlen = tokens.shape
        h = self.tok_embeddings(tokens)  # (bsz, partial_seqlen, dim)
        self.freqs_cis = self.freqs_cis.to(h.device)
        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]

        mask = None
        if seqlen > 1:
            mask = torch.full((1, 1, seqlen, seqlen), float("-inf"), device=tokens.device)
            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)

        for layer in self.layers:
            h = layer(h, start_pos, freqs_cis, mask)
            
        h = self.norm(h)
        output = self.output(h)
        if last_logits_only: return output.float()[:,-1,:]
        return output.float()

# %% ../nbs/01_model.ipynb 14
class FunctionModel(nn.Module):
    def __init__(self, model, func_dict, inference_mode="func_embedding", load_embed_path=None):
        super().__init__()
        fc.store_attr()
        self.func_list = {v: k for k, v in func_dict.items()}
        self.func_embed = nn.Linear(model.params.dim, len(func_dict), bias=False).cuda()
        self.logits_bias = 0
        if load_embed_path is not None:
            embedding = torch.load(load_embed_path, map_location='cuda')
            if embedding['weight'].shape[0] > len(func_dict):
                embedding = embedding[:len(func_dict)]
            self.func_embed.load_state_dict(embedding)

    def set_bias(self, logits_bias):
        self.logits_bias = logits_bias
        
    def forward(self, tokens: torch.Tensor, start_pos: int):
        with torch.no_grad():
            last_logits, h = self.model(tokens, start_pos)
            token_logits = self.model.output(h)
        func_logits = self.func_embed(h.float())
        if self.inference_mode != 'func_embedding':
            func_logits = torch.zeros_like(func_logits) - 1e5
        return torch.cat([token_logits, func_logits], dim=-1)

# %% ../nbs/01_model.ipynb 15
def sample_top_p(probs, p):
    if len(probs.shape) == 3:
        if probs.shape[-1]*probs.shape[-2] >= 2**24:
            probs = probs[:,-1,:]
        else: probs = rearrange(probs, 'n s d -> n (s d)')
    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sum - probs_sort > p
    probs_sort[mask] = 0.0
    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
    next_token = torch.multinomial(probs_sort, num_samples=1)
    next_token = torch.gather(probs_idx, -1, next_token)
    return next_token

# %% ../nbs/01_model.ipynb 16
def setup_model_parallel() -> Tuple[int, int]:
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    world_size = int(os.environ.get("WORLD_SIZE", -1))

    torch.distributed.init_process_group("nccl")
    initialize_model_parallel(world_size)
    torch.cuda.set_device(local_rank)
    return local_rank, world_size
