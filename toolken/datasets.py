# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_datasets.ipynb.

# %% ../nbs/03_datasets.ipynb 2
from __future__ import annotations
import math, random, torch, matplotlib.pyplot as plt, numpy as np, matplotlib as mpl, shutil, os, gzip, pickle, re, copy, time
from pathlib import Path
from functools import partial
import fastcore.all as fc
from glob import glob
import json

from torch import tensor, nn, optim
import torch.nn.functional as F
from datasets import load_dataset
from torch.optim import lr_scheduler
from torch.utils.data import DataLoader, default_collate
from torch.nn import init
from torch.nn.utils.rnn import pad_sequence
from typing import List, Optional

from datetime import datetime, timedelta
import calendar
from fastprogress import progress_bar
from einops import rearrange

from .model import *
from .tokenizer import *

# %% auto 0
__all__ = ['PromptDS', 'DataLoaders']

# %% ../nbs/03_datasets.ipynb 8
class PromptDS:
    def __init__(self, json, tokenizer, func_dict): fc.store_attr()
    def __len__(self): return len(self.json)
    def __getitem__(self, i): 
        item = self.json[i]
        item['input'] = tensor(self.tokenizer.encode(item['text'], bos=True, eos=True))
        item['label'] = tensor(self.tokenizer.encode(item['text'], bos=True, eos=True))
        for i, idx in enumerate(item['start_token_idx']):
            start, end = idx, item['end_token_idx'][i]
            op = re.search(r"(<.*?>)", item['tar_eq'][i]).group(1)
            item['label'][start] = self.func_dict[op] + 32000
            item['label'][start+1:end] = -100
        return item['input'], item['label']

# %% ../nbs/03_datasets.ipynb 9
class DataLoaders:
    def __init__(self, tds, vds, bs, **kwargs): 
        self.train = DataLoader(tds, batch_size=bs, shuffle=True, collate_fn=default_collate, num_workers=4, **kwargs)
        self.valid = DataLoader(vds, batch_size=bs, collate_fn=default_collate, num_workers=4, **kwargs)
