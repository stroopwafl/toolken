{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc27b6a1-852e-4526-a2d5-339061e391a1",
   "metadata": {},
   "source": [
    "# Train functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e990b15-887e-46eb-9020-a75a8d070a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6496ab3b-54f0-413b-9e64-d37ead6166fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import math, random, torch, matplotlib.pyplot as plt, numpy as np, matplotlib as mpl, shutil, os, gzip, pickle, re, copy, time\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import fastcore.all as fc\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "from torch import tensor, nn, optim\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch.nn import init\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import List, Optional\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "from fastprogress import progress_bar\n",
    "from einops import rearrange\n",
    "\n",
    "from toolken.model import *\n",
    "from toolken.tokenizer import *\n",
    "from toolken.datasets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ceed09-6ba5-413f-84d7-ea14b7a48afe",
   "metadata": {},
   "source": [
    "Below is a basic training object that trains Toolken embeddings, stores the training results in a JSON file and saves the embeddings to a pytorch file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c312f9-26f2-4595-a120-e6fa76bdcb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Trainer:\n",
    "    def __init__(self, dls, model, tokenizer, save_path, epochs=10, lr=1e-3, device='cuda', opt_func=optim.Adam):\n",
    "        fc.store_attr()\n",
    "        if not self.save_path.endswith('/'): self.save_path += '/'\n",
    "    def train_embeddings(self):\n",
    "        self.all_results = {}\n",
    "        opt = self.opt_func([p for p in self.model.parameters() if p.requires_grad], self.lr)\n",
    "        for self.epoch in progress_bar(range(self.epochs)):\n",
    "            \n",
    "            self.make_storage()\n",
    "\n",
    "            # do train\n",
    "            for batch in progress_bar(self.dls.train, leave=False):\n",
    "                self.model.train()\n",
    "                inp, label = batch[0].to(self.device), batch[1].to(self.device)\n",
    "                concat_logits = self.model(inp, 0)\n",
    "                loss = F.cross_entropy(concat_logits.view(-1, concat_logits.shape[-1]), label.view(-1), ignore_index=-100)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "                self.all_results[self.epoch]['train_losses'].append(loss.detach().cpu().numpy().item())\n",
    "\n",
    "            # do valid\n",
    "            for batch in progress_bar(self.dls.valid, leave=False):\n",
    "                self.model.eval()\n",
    "                inp, label = batch[0].to(self.device), batch[1].to(self.device)\n",
    "                concat_logits = self.model(inp, 0)\n",
    "                loss = F.cross_entropy(concat_logits.view(-1, concat_logits.shape[-1]), label.view(-1), ignore_index=-100)\n",
    "                self.all_results[self.epoch]['eval_losses'].append(loss.detach().cpu().numpy().item())\n",
    "                self.calculate_results(concat_logits, label)\n",
    "            \n",
    "            self.print_epoch_results()\n",
    "            self.save_embeddings()\n",
    "            self.save_results()\n",
    "    \n",
    "    def make_storage(self):\n",
    "        self.all_results[self.epoch] = {\n",
    "            \"train_losses\": [],\n",
    "            \"eval_losses\": [],\n",
    "            \"eval_matches\": [],\n",
    "            \"eval_predicted\": [],\n",
    "            \"eval_real\": [],\n",
    "            \"eval_pc\": []\n",
    "        }\n",
    "    def calculate_results(self, concat_logits, label):\n",
    "        func_dict = self.model.func_dict\n",
    "        pred = torch.argmax(concat_logits, dim=-1).squeeze()\n",
    "        label_funcs = torch.stack([label == func_dict[op] + 32000 for op in func_dict.keys()])\n",
    "        pred_funcs = torch.stack([pred == func_dict[op] + 32000 for op in func_dict.keys()])\n",
    "        \n",
    "        matches = torch.sum(label_funcs * pred_funcs, dim=-1).detach().cpu().numpy()\n",
    "        pred_funcs = torch.sum(pred_funcs, dim=-1).detach().cpu().numpy()\n",
    "        real = torch.sum(label_funcs, dim=-1).detach().cpu().numpy()\n",
    "        pc = matches / (real + 1e-8)\n",
    "\n",
    "        for key, res in zip(list(self.all_results[self.epoch].keys())[-4:], [matches, pred_funcs, real, pc]):\n",
    "            self.all_results[self.epoch][key].append(res.tolist())\n",
    "    def print_epoch_results(self):\n",
    "        train_loss = np.array(self.all_results[self.epoch]['train_losses']).mean()\n",
    "        eval_loss = np.array(self.all_results[self.epoch]['eval_losses']).mean()\n",
    "        eval_pc = np.array(self.all_results[self.epoch]['eval_pc']).mean()\n",
    "        print(f'Epoch {self.epoch} || Train loss: {train_loss} || Eval loss: {eval_loss} || Eval pc: {eval_pc}')\n",
    "    def save_embeddings(self):\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "        torch.save(self.model.func_embed.state_dict(), f\"{self.save_path}/epoch_{self.epoch}.pth\")\n",
    "    def save_results(self):\n",
    "        import pdb; pdb.set_trace()\n",
    "        with open(f'{self.save_path}results.json', 'w') as file:\n",
    "            json.dump(self.all_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1213334f-8a66-4865-b747-759d891af11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e51097-9348-4f0a-8c2d-33ee7f42be8b",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc743b8-74e8-4ee7-b472-bd6e845ccf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['LOCAL_RANK'] = '0'\n",
    "# os.environ['WORLD_SIZE'] = '1'\n",
    "# os.environ['RANK'] = '0'\n",
    "# os.environ['MASTER_ADDR'] = '172.17.0.3'\n",
    "# os.environ['MASTER_PORT'] = '6006'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461a25d-e3be-4fae-bf1f-7324bfcd9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_rank, world_size = setup_model_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee72d2-7186-47f8-ab5e-47e1077b4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '../model/tokenizer.model'\n",
    "# tokenizer = Tokenizer(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6b681a-92d5-4870-870b-0fa16c682d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/models/foundation/LLaMA/7B'\n",
    "# checkpoint = torch.load(f'{path}/consolidated.00.pth')\n",
    "# with open(Path(path) / \"params.json\", \"r\") as f: params = json.loads(f.read())\n",
    "# model_args = ModelArgs(max_seq_len=2048, max_batch_size=8, **params)\n",
    "# model_args.vocab_size = tokenizer.n_words\n",
    "# model = Transformer(model_args).cuda().half()\n",
    "# torch.set_default_tensor_type(torch.FloatTensor)\n",
    "# model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae62a0f-c74c-42f7-a927-90354dbd4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/gsm8k-xl/func_dict.json', 'r') as f: func_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59046e-e7dd-432a-a3b5-9b064b4f9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmodel = FunctionModel(model, func_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b96f1a-3757-4746-bbe0-b5ee9568fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len([p for p in fmodel.parameters() if p.requires_grad]) / len([p for p in fmodel.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e212c7-30b1-4c54-b076-5bbd481b1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/gsm8k-xl/train.json', 'r') as f: data = json.load(f)\n",
    "# tds = PromptDS(data[:int(0.9*len(data))], tokenizer, func_dict)\n",
    "# vds = PromptDS(data[int(0.9*len(data)):], tokenizer, func_dict)\n",
    "# dls = DataLoaders(tds, vds, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33478ca-ecb9-4909-b47c-23ebffd37a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/projects/toolken/'\n",
    "# trainer = Trainer(dls, fmodel, tokenizer, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb4443-3a26-4ebe-8935-a5f96b2b4f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ccd7aa-691c-4291-bf6b-4e7c9a3c2a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
