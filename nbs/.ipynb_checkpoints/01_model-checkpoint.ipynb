{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff801ddf-a4a2-4f2e-9efa-12c2a23e09ac",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d55f8-734a-491c-9f51-7b0a6126e5d8",
   "metadata": {},
   "source": [
    "This is just a slightly adapted version of the original LLaMA 1 code available at [this repo](https://github.com/facebookresearch/llama/tree/main/llama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbbd3c-a5ca-4ba0-9520-06e81e801b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2eb64-42a0-443f-b5ad-c19e9984dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from typing import List\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import random\n",
    "import json\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "from fairscale.nn.model_parallel.layers import (\n",
    "    ParallelEmbedding,\n",
    "    RowParallelLinear,\n",
    "    ColumnParallelLinear,\n",
    ")\n",
    "from fairscale.nn.model_parallel.mappings import scatter_to_model_parallel_region, gather_from_model_parallel_region\n",
    "from fairscale.nn.model_parallel.initialize import initialize_model_parallel\n",
    "import re\n",
    "import fastcore.all as fc\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982bc4a1-1577-402f-aa68-78dbe44c3f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 512\n",
    "    n_layers: int = 8\n",
    "    n_heads: int = 8\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "    lora = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f0a3b-d4b4-4c80-abce-81191a10ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac556407-dfbc-437a-8848-1491ddc53c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90613767-1b0b-40aa-bd84-f6562748de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0544d135-e63c-441a-ab9b-e7ff8cb4e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b02a30e-ceb7-44dd-b7db-09cfc0ab7f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LoRA(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.lora_alpha = 32\n",
    "        self.r = 4\n",
    "        self.scaling = self.lora_alpha / self.r\n",
    "\n",
    "        self.lora_A = nn.Parameter(torch.zeros((self.r, input_dim)))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((output_dim, self.r)))\n",
    "        self.reset_lora_parameters()\n",
    "\n",
    "    def reset_lora_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
    "\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "        \n",
    "        prefix = name + '.lora_A'\n",
    "        if prefix not in state_dict:\n",
    "            state_dict[prefix] = self.lora_A\n",
    "\n",
    "        prefix = name + '.lora_B'\n",
    "        if prefix not in state_dict:\n",
    "            state_dict[prefix] = self.lora_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab77f47-a34e-4944-91a4-45bb49e8c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_local_heads = args.n_heads // fs_init.get_model_parallel_world_size()\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.lora = args.lora\n",
    "\n",
    "        self.wq = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wk = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wv = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wo = RowParallelLinear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "            input_is_parallel=True,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        ).cuda()\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        ).cuda()\n",
    "        \n",
    "        if self.lora:\n",
    "            self.q_lora = LoRA(args.dim, args.dim)\n",
    "            self.k_lora = LoRA(args.dim, args.dim)\n",
    "            self.v_lora = LoRA(args.dim, args.dim)\n",
    "                \n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        \n",
    "        if self.lora:\n",
    "            q = gather_from_model_parallel_region(q) + self.q_lora(x)\n",
    "            k = gather_from_model_parallel_region(k) + self.k_lora(x)\n",
    "            v = gather_from_model_parallel_region(v) + self.v_lora(x)\n",
    "\n",
    "            q = scatter_to_model_parallel_region(q)\n",
    "            k = scatter_to_model_parallel_region(k)\n",
    "            v = scatter_to_model_parallel_region(v)\n",
    "\n",
    "        q = q.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        k = k.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        v = v.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "\n",
    "        q, k = apply_rotary_emb(q, k, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(q)\n",
    "        self.cache_v = self.cache_v.to(q)\n",
    "        \n",
    "        cache_k = self.cache_k.detach().clone()\n",
    "        cache_v = self.cache_v.detach().clone()\n",
    "        cache_k[:bsz, start_pos : start_pos + seqlen] = k\n",
    "        cache_v[:bsz, start_pos : start_pos + seqlen] = v\n",
    "        self.cache_k = cache_k\n",
    "        self.cache_v = cache_v\n",
    "\n",
    "        # self.cache_k[:bsz, start_pos : start_pos + seqlen] = k\n",
    "        # self.cache_v[:bsz, start_pos : start_pos + seqlen] = v\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(q, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(q)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim)\n",
    "        output = output.transpose(\n",
    "            1, 2\n",
    "        ).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6175e54-b401-4dcf-bc01-aa98ef60c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "        self.w2 = RowParallelLinear(\n",
    "            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n",
    "        )\n",
    "        self.w3 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7d1dd-4dd2-4fed-b42f-ec0457d06493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c1c71e-ce50-4c4c-aea8-6bbc574d98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = ParallelEmbedding(\n",
    "            params.vocab_size, params.dim, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = ColumnParallelLinear(\n",
    "            params.dim, params.vocab_size, bias=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "        )\n",
    "\n",
    "    # @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int, last_logits_only=False):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)  # (bsz, partial_seqlen, dim)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "            \n",
    "        h = self.norm(h)\n",
    "        output = self.output(h)\n",
    "        if last_logits_only: return output.float()[:,-1,:]\n",
    "        return output.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228958c0-1d73-427c-bd53-2a2e4d48086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FunctionModel(nn.Module):\n",
    "    def __init__(self, model, func_dict, inference_mode=\"func_embedding\", load_embed_path=None):\n",
    "        super().__init__()\n",
    "        fc.store_attr()\n",
    "        self.func_list = {v: k for k, v in func_dict.items()}\n",
    "        self.func_embed = nn.Linear(model.params.dim, len(func_dict), bias=False).cuda()\n",
    "        self.logits_bias = 0\n",
    "        if load_embed_path is not None:\n",
    "            embedding = torch.load(load_embed_path, map_location='cuda')\n",
    "            if embedding['weight'].shape[0] > len(func_dict):\n",
    "                embedding = embedding[:len(func_dict)]\n",
    "            self.func_embed.load_state_dict(embedding)\n",
    "        self.init_grads()\n",
    "\n",
    "    def set_bias(self, logits_bias):\n",
    "        self.logits_bias = logits_bias\n",
    "        \n",
    "    def init_grads(self):\n",
    "        for p in self.model.parameters(): p.requires_grad = False\n",
    "        for p in self.func_embed.parameters(): p.requires_grad = True\n",
    "        \n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        with torch.no_grad():\n",
    "            last_logits, h = self.model(tokens, start_pos)\n",
    "            token_logits = self.model.output(h)\n",
    "        func_logits = self.func_embed(h.float())\n",
    "        if self.inference_mode != 'func_embedding':\n",
    "            func_logits = torch.zeros_like(func_logits) - 1e5\n",
    "        return torch.cat([token_logits, func_logits], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f0f552-8c63-4920-bea0-cf826943841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_top_p(probs, p):\n",
    "    if len(probs.shape) == 3:\n",
    "        if probs.shape[-1]*probs.shape[-2] >= 2**24:\n",
    "            probs = probs[:,-1,:]\n",
    "        else: probs = rearrange(probs, 'n s d -> n (s d)')\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a3d7af-928e-4bea-9bf2-69ea0a6d22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_model_parallel() -> Tuple[int, int]:\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", -1))\n",
    "\n",
    "    torch.distributed.init_process_group(\"nccl\")\n",
    "    initialize_model_parallel(world_size)\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    return local_rank, world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39369ad5-7773-4032-b25c-1b676dfcb0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27937a05-94cc-4b69-9900-c9e6070760b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d05ed4-3833-4413-a70a-57df87a0cb37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
